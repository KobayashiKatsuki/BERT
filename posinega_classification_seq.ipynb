{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b8c9029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "from transformers import BertJapaneseTokenizer, BertForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "MODEL_NAME = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e31bfb1",
   "metadata": {},
   "source": [
    "### 事前準備\n",
    "学習データ成形、事前学習クラスのロード、BERTを使ったモデルのクラス作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f94db0f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>当社グループを取り巻く環境は、実質賃金が伸び悩むなか、消費者の皆様の生活防衛意識の高まりや節...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>春から夏にかけましては個人消費の低迷などにより、きのこの価格は厳しい状況で推移いたしました</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>台湾の現地法人「台灣北斗生技股份有限公司」におきましては、ブランドの構築、企画提案などに力を...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>化成品事業におきましては、引き続き厳しい販売環境にありましたが、中核である包装資材部門におき...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>以上の結果、化成品事業の売上高は92億45百万円（同1.7％減）となりました</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2808</th>\n",
       "      <td>当連結会計年度におきましては、連結子会社のデジタル・アドバタイジング・コンソーシアム株式会社...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2809</th>\n",
       "      <td>新規の自動ドアの売上台数は僅かに減少したものの、シートシャッターの大型物件に加え、取替の売上...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2810</th>\n",
       "      <td>加えて、保守契約が堅調に増加し、売上高は6,952百万円（前年同期比1.2％増）となりました</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2811</th>\n",
       "      <td>利益につきましては、取替工事の増加及び保守契約による安定的な利益の確保により、セグメント利益...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2812</th>\n",
       "      <td>その他のセグメントでは駐輪システムが堅調に推移し、売上高は721百万円（前年同期比0.8％増...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2813 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "0     当社グループを取り巻く環境は、実質賃金が伸び悩むなか、消費者の皆様の生活防衛意識の高まりや節...      0\n",
       "1         春から夏にかけましては個人消費の低迷などにより、きのこの価格は厳しい状況で推移いたしました      0\n",
       "2     台湾の現地法人「台灣北斗生技股份有限公司」におきましては、ブランドの構築、企画提案などに力を...      0\n",
       "3     化成品事業におきましては、引き続き厳しい販売環境にありましたが、中核である包装資材部門におき...      0\n",
       "4                以上の結果、化成品事業の売上高は92億45百万円（同1.7％減）となりました      0\n",
       "...                                                 ...    ...\n",
       "2808  当連結会計年度におきましては、連結子会社のデジタル・アドバタイジング・コンソーシアム株式会社...      1\n",
       "2809  新規の自動ドアの売上台数は僅かに減少したものの、シートシャッターの大型物件に加え、取替の売上...      1\n",
       "2810     加えて、保守契約が堅調に増加し、売上高は6,952百万円（前年同期比1.2％増）となりました      1\n",
       "2811  利益につきましては、取替工事の増加及び保守契約による安定的な利益の確保により、セグメント利益...      1\n",
       "2812  その他のセグメントでは駐輪システムが堅調に推移し、売上高は721百万円（前年同期比0.8％増...      1\n",
       "\n",
       "[2813 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ポジネガデータセット\n",
    "df_dataset = pd.read_csv(\n",
    "    'D:/DataSet/chABSA-dataset/chABSA-dataset/dataset.tsv',\n",
    "    sep='\\t', \n",
    "    header=None\n",
    ").rename(columns={0:'text', 1:'label'}).loc[:, ['text', 'label']]\n",
    "\n",
    "# ひとまずこういうFmtのデータに成形するところまでがんばる\n",
    "df_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "545fc1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "https://dreamer-uma.com/pytorch-dataset/\n",
    "\n",
    "対象タスクのデータを扱うDataset\n",
    "データの格納と引き出し　DataLoaderと組み合わせてミニバッチ学習が可能\n",
    "Datasetを自作する場合は必ず以下のメソッドを実装すること\n",
    "__len__(): Datasetのサイズ（データ数）\n",
    "__getitem__(): Datasetの要素にアクセス\n",
    "\n",
    "\"\"\"\n",
    "class PosiNegaDataset(Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = { k: torch.tensor(v[idx]) for k, v in self.encodings.items() }\n",
    "#         item = { k: torch.tensor(v[idx]).cuda() for k, v in self.encodings.items() }\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9d016d",
   "metadata": {},
   "source": [
    "### 事前準備、以上\n",
    "ここからは上で用意した各種クラスやモデルやデータセットを使ってタスクを解くコーディングをしていく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4255319a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# トークナイザ\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# 事前学習モデル\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "# model = model.cuda()  \n",
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6d505ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴量X、ラベルyを取得\n",
    "X, y = df_dataset[\"text\"].values, df_dataset[\"label\"].values\n",
    "\n",
    "# train, val分割\n",
    "# random_stateはシャッフルの乱数シード固定、stratifyは正例、負例のラベル数均一にする処理\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5, random_state=0, stratify=y_val)\n",
    "\n",
    "\n",
    "# トークナイザでモデルへのinputとなるようencodingする\n",
    "max_len = 256 #512\n",
    "\n",
    "enc_train = tokenizer(\n",
    "    X_train.tolist(), \n",
    "    add_special_tokens=True, \n",
    "    max_length=max_len,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_tensors='pt',\n",
    ")\n",
    "# tokenizer.convert_ids_to_tokens(enc_train['attention_masks'].tolist()[0])\n",
    "\n",
    "enc_val = tokenizer(\n",
    "    X_val.tolist(), \n",
    "    add_special_tokens=True, \n",
    "    max_length=max_len,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_tensors='pt',\n",
    ")\n",
    "\n",
    "enc_test = tokenizer(\n",
    "    X_test.tolist(), \n",
    "    add_special_tokens=True, \n",
    "    max_length=max_len,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_tensors='pt',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e26812a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasetを作成\n",
    "ds_train = PosiNegaDataset(enc_train, y_train)\n",
    "ds_val = PosiNegaDataset(enc_val, y_val)\n",
    "\n",
    "# DataLoaderを作成\n",
    "batch_size = 8\n",
    "dl_train = DataLoader(ds_train, batch_size=batch_size, shuffle=True)\n",
    "dl_val = DataLoader(ds_val, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1d6597f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_matricsを定義\n",
    "def compute_metrics(p):\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred)\n",
    "    precision = precision_score(y_true=labels, y_pred=pred)\n",
    "    f1 = f1_score(y_true=labels, y_pred=pred)\n",
    "    \n",
    "    return {\"accuracy\": accuracy, \"recall\": recall, \"precision\": precision, \"f1\": f1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da37bc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainerを作成\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./outputs',\n",
    "    num_train_epochs=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    no_cuda=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=ds_val,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70da72a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\licht\\anaconda3\\envs\\py39\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2250\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 282\n",
      "C:\\Users\\licht\\AppData\\Local\\Temp\\ipykernel_4876\\4088419975.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = { k: torch.tensor(v[idx]) for k, v in self.encodings.items() }\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [282/282 01:31, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=282, training_loss=0.423732172724203, metrics={'train_runtime': 95.775, 'train_samples_per_second': 23.493, 'train_steps_per_second': 2.944, 'total_flos': 295999937280000.0, 'train_loss': 0.423732172724203, 'epoch': 1.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ファインチューニング\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070ba2d0",
   "metadata": {},
   "source": [
    "### モデルのテスト\n",
    "testデータを使って推論の精度確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a93555a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./outputs/checkpoint-500\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"cl-tohoku/bert-base-japanese-whole-word-masking\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"tokenizer_class\": \"BertJapaneseTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file ./outputs/checkpoint-500\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ./outputs/checkpoint-500.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 282\n",
      "  Batch size = 8\n",
      "C:\\Users\\licht\\AppData\\Local\\Temp\\ipykernel_4876\\4088419975.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = { k: torch.tensor(v[idx]) for k, v in self.encodings.items() }\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [36/36 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# データセット\n",
    "ds_test = PosiNegaDataset(enc_test, y_test)\n",
    "\n",
    "# モデル読み込み\n",
    "model_path = \"./outputs/checkpoint-500\"\n",
    "trained_model = BertForSequenceClassification.from_pretrained(model_path, num_labels=2)\n",
    "test_trainer = Trainer(trained_model)\n",
    "\n",
    "# 推論\n",
    "raw_pred, _, _ = test_trainer.predict(ds_test)\n",
    "\n",
    "y_pred = np.argmax(raw_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5c802c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9148936170212766 0.874251497005988 0.9798657718120806 0.9240506329113924\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "recall = recall_score(y_true=y_test, y_pred=y_pred)\n",
    "precision = precision_score(y_true=y_test, y_pred=y_pred)\n",
    "f1 = f1_score(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "print(accuracy, recall, precision, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf999d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[112   3]\n",
      " [ 21 146]]\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492a7386",
   "metadata": {},
   "source": [
    "### 実用フェーズ\n",
    "作成したモデルで推論してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "738f8788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 当社の売り上げは１０年連続減少、経営黒字が続いている\n"
     ]
    }
   ],
   "source": [
    "# テスト用トークナイザ\n",
    "tokenizer_for_test = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n",
    "# ファインチューニング済みモデル\n",
    "model_path = \"./outputs/checkpoint-500\"\n",
    "trained_model = BertForSequenceClassification.from_pretrained(model_path, num_labels=2)\n",
    "\n",
    "# 使ってみる\n",
    "text = input('> ')\n",
    "\n",
    "encoding = tokenizer_for_test( \n",
    "    text,\n",
    "    add_special_tokens=True, \n",
    "    max_length=256,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "#print(tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"].tolist()[0]))\n",
    "\n",
    "if trained_model.device.type == 'cuda':\n",
    "    encoding = { k: v.cuda() for k, v in encoding.items() }\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = trained_model(**encoding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12190cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分類スコア\n",
      "tensor([[ 2.0845, -2.4853]])\n",
      "確率\n",
      "tensor([[0.9897, 0.0103]])\n",
      "推論結果\n",
      "tensor([0])\n"
     ]
    }
   ],
   "source": [
    "# 分類スコア\n",
    "scores = output.logits\n",
    "# 確率\n",
    "prob = scores.softmax(dim=1)\n",
    "# 予測ラベル\n",
    "predicted_labels = scores.argmax(-1)\n",
    "\n",
    "print(\"分類スコア\")\n",
    "print(scores)\n",
    "print(\"確率\")\n",
    "print(prob)\n",
    "print(\"推論結果\")\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea13a532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分類スコア\n",
      "tensor([[ 2.1422, -2.4919],\n",
      "        [-0.4833,  2.7143],\n",
      "        [-0.5637,  2.2063]])\n",
      "確率\n",
      "tensor([[0.9904, 0.0096],\n",
      "        [0.0393, 0.9607],\n",
      "        [0.0590, 0.9410]])\n",
      "推論結果\n",
      "tensor([0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    '当社の売り上げは１０年連続減少、経営赤字が続いている',\n",
    "    '新製品の開発に成功、収益は過去最高を達成しました',\n",
    "    'あの映画は本当に面白いからぜひ見てね',    \n",
    "]\n",
    "encoding = tokenizer_for_test( \n",
    "    texts,\n",
    "    add_special_tokens=True, \n",
    "    max_length=256,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "if trained_model.device.type == 'cuda':\n",
    "    encoding = { k: v.cuda() for k, v in encoding.items() }\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = trained_model(**encoding)\n",
    "\n",
    "scores = output.logits\n",
    "prob = scores.softmax(dim=1)\n",
    "predicted_labels = scores.argmax(-1)\n",
    "print(\"分類スコア\")\n",
    "print(scores)\n",
    "print(\"確率\")\n",
    "print(prob)\n",
    "print(\"推論結果\")\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374eee39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
