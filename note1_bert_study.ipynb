{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b7f5a5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "BERTの勉強 note1\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import glob, pickle\n",
    "\n",
    "pretrained_model_name = \"cl-tohoku/bert-base-japanese\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a27265b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ読み込み\n",
    "# 事前にトークナイズして保存しておいてもいい\n",
    "df_train = pd.read_csv(\"../../DataSet/ldcc/reshaped/train.tsv\", sep=\"\\t\", header=None)\n",
    "df_valid = pd.read_csv(\"../../DataSet/ldcc/reshaped/valid.tsv\", sep=\"\\t\", header=None)\n",
    "df_test = pd.read_csv(\"../../DataSet/ldcc/reshaped/test.tsv\", sep=\"\\t\", header=None)\n",
    "\n",
    "\n",
    "# textとlabelに分ける\n",
    "text_train, labels_train = list(df_train[0].values), list(df_train[1].values)\n",
    "text_valid, labels_valid = list(df_valid[0].values), list(df_valid[1].values)\n",
    "text_test,  labels_test  = list(df_test[0].values),  list(df_test[1].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "478c4def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# トークナイザを事前モデルからロード\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2f9b360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# トークナイズ処理\n",
    "enc_train = tokenizer(text_train, truncation=True, padding=True)\n",
    "enc_valid = tokenizer(text_valid, truncation=True, padding=True)\n",
    "enc_test = tokenizer(text_test, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da757827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# タスク用Datasetクラスを定義\n",
    "class LivedoorDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = { key: torch.tensor(val[idx]) for key, val in self.encodings.items() }\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx]) # item[\"label\"]でなくitem[\"labels\"]が正しい！\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a77c8075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasetを作成\n",
    "ds_train = LivedoorDataset(enc_train, labels_train)\n",
    "ds_valid = LivedoorDataset(enc_valid, labels_valid)\n",
    "ds_test = LivedoorDataset(enc_test, labels_test)\n",
    "\n",
    "# pkl保存\n",
    "with open(\"../../DataSet/ldcc/dataloader/ds_train.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ds_train, f)\n",
    "with open(\"../../DataSet/ldcc/dataloader/ds_valid.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ds_valid, f)\n",
    "with open(\"../../DataSet/ldcc/dataloader/ds_test.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ds_test, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38017826",
   "metadata": {},
   "source": [
    "#### データの準備ここまで\n",
    "ここからはファインチューニング用にモデル作る\\\n",
    "自作だろうがTransformersのモデル使おうが上記までは共通事項"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92bea57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_train = 16\n",
    "batch_size_val = 64\n",
    "\n",
    "bt_train = DataLoader(ds_train, batch_size=batch_size_train)\n",
    "bt_val = DataLoader(ds_valid, batch_size=batch_size_val)\n",
    "dataloader_dict = {\"train\": bt_train, \"val\": bt_val}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "792f33fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ファインチューニング用モデル\n",
    "\"\"\"\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_model):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        \n",
    "        self.bert = pretrained_model\n",
    "        self.dropout = nn.Dropout(p=.1)\n",
    "        self.classifier = nn.Linear(in_features=768, out_features=9) #9カテゴリのクラス分類\n",
    "        \n",
    "        # 重み初期化\n",
    "        nn.init.normal_(self.classifier.weight, std=.02)\n",
    "        nn.init.normal_(self.classifier.bias, 0)\n",
    "        \n",
    "    def forward(self, input_ids, labels=None, **kwargs):\n",
    "        output = self.bert(input_ids)\n",
    "        pooler_output = output.pooler_output\n",
    "        pooler_output = self.dropout(pooler_output)\n",
    "        output_classifier = self.classifier(pooler_output)\n",
    "        return output_classifier        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9da64bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# 事前学習モデルを用意\n",
    "pretrained_model = AutoModel.from_pretrained(pretrained_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ed07154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertClassifier(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 自作Bertファインチューニングモデル\n",
    "my_model = BertClassifier(pretrained_model)\n",
    "\n",
    "my_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3b85d1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算時間短縮・モデルの性能低下の防止を目的に事前学習のレイヤを学習に関与しないようにする（＝Freeze）ことがある\n",
    "# Freezeのやり方は各レイヤのparameters()で取り出せるparam.requires_gradをT/FでOn/Offする\n",
    "\n",
    "# これにより全パラメータ固定\n",
    "for param in my_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# BERTの最終層の更新をON\n",
    "for param in my_model.bert.encoder.layer[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "# ドロップアウト層の更新をON\n",
    "for param in my_model.dropout.parameters():\n",
    "    param.requires_grad = True\n",
    "# クラス分類層の更新をON\n",
    "for param in my_model.classifier.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f430204a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最適化関数（重み更新式）の定義 Adam使う\n",
    "optimizer = optim.Adam([\n",
    "    {\"params\": my_model.bert.encoder.layer[-1].parameters(), \"lr\":5e-5},\n",
    "    {\"params\": my_model.dropout.parameters(), \"lr\":1e-3},\n",
    "    {\"params\": my_model.classifier.parameters(), \"lr\":1e-4},\n",
    "])\n",
    "\n",
    "# 損失関数　クラス分類なのでCrossEntropy\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99576d04",
   "metadata": {},
   "source": [
    "#### モデルの定義ここまで\n",
    "ここからは学習の工程を定義する\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b100b475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(net, dataloader_dict, criterion, optimizer, num_epochs):\n",
    "    # GPU利用可能ならそうする\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net.to(device) # モデルをGPU or CPUに送る\n",
    "    \n",
    "    torch.backends.cudnn.benchmark = True # 高速化するらしい？\n",
    "    \n",
    "    # 以下、epochループ\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        # train と val をそれぞれ実施\n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            batch_size = dataloader_dict[phase].batch_size\n",
    "            if phase == \"train\":\n",
    "                net.train()\n",
    "            else:\n",
    "                net.eval()\n",
    "            \n",
    "            epoch_loss = .0\n",
    "            epoch_corrects = 0\n",
    "            iteration = 1\n",
    "            \n",
    "            for batch in (dataloader_dict[phase]): # ミニバッチとしてデータ取り出す\n",
    "                inputs = batch[\"input_ids\"].to(device) # GPU or CPUへデータ送る\n",
    "                labels = batch[\"labels\"].to(device) # GPU or CPUへデータ送る\n",
    "                \n",
    "                optimizer.zero_grad() # optimizer初期化\n",
    "                \n",
    "                # feed forward処理\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    outputs = net(inputs)\n",
    "                    loss = criterion(outputs, labels) # nn.CrossEntropyLossは内部でSoftmax相当の処理をするのでoutput直でOK\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    \n",
    "                    if phase == \"train\": # trainモードならback prop\n",
    "                        loss.backward() # 損失から勾配計算しパラメータへ逆伝播\n",
    "                        optimizer.step() # パラメータ更新\n",
    "                    \n",
    "                        if (iteration % 10 == 0):\n",
    "                            acc = (torch.sum(preds == labels.data)).double() / batch_size\n",
    "                            print(f\"It:{iteration:3d}|Loss: {loss.item():.4f}|accuracy:{acc:.4f}\")\n",
    "                \n",
    "                iteration += 1\n",
    "                \n",
    "                epoch_loss += loss.item() * batch_size\n",
    "                epoch_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "        # epoch毎に評価結果を出力\n",
    "        epoch_loss = epoch_loss / len(dataloader_dict[phase].dataset)\n",
    "        epoch_acc = epoch_corrects.double() / len(dataloader_dict[phase].dataset)\n",
    "        print(f\"Epoch {epoch+1} / {num_epochs} | {phase} | Loss: {epoch_loss:.4f} | Acc: {epoch_acc}\")\n",
    "    \n",
    "    return net #訓練後のモデルを出力    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "54666924",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It: 10|Loss: 0.5591|accuracy:0.8125\n",
      "It: 20|Loss: 0.3166|accuracy:0.8750\n",
      "It: 30|Loss: 0.0837|accuracy:1.0000\n",
      "It: 40|Loss: 0.3079|accuracy:0.9375\n",
      "It: 50|Loss: 0.4084|accuracy:0.8125\n",
      "It: 60|Loss: 0.1415|accuracy:0.9375\n",
      "It: 70|Loss: 0.5575|accuracy:0.8125\n",
      "It: 80|Loss: 0.3694|accuracy:0.8750\n",
      "It: 90|Loss: 0.1939|accuracy:0.9375\n",
      "It:100|Loss: 0.4419|accuracy:0.8750\n",
      "It:110|Loss: 0.1446|accuracy:1.0000\n",
      "It:120|Loss: 0.5115|accuracy:0.8750\n",
      "It:130|Loss: 0.6629|accuracy:0.6875\n",
      "It:140|Loss: 0.3873|accuracy:0.8750\n",
      "It:150|Loss: 0.2579|accuracy:0.9375\n",
      "It:160|Loss: 0.1507|accuracy:0.9375\n",
      "It:170|Loss: 0.3413|accuracy:0.9375\n",
      "It:180|Loss: 0.3615|accuracy:0.8750\n",
      "It:190|Loss: 0.2028|accuracy:0.9375\n",
      "It:200|Loss: 0.2679|accuracy:0.8750\n",
      "It:210|Loss: 0.2646|accuracy:0.8750\n",
      "It:220|Loss: 0.1433|accuracy:0.9375\n",
      "It:230|Loss: 0.4972|accuracy:0.7500\n",
      "It:240|Loss: 0.1533|accuracy:1.0000\n",
      "It:250|Loss: 0.3262|accuracy:0.9375\n",
      "It:260|Loss: 0.0727|accuracy:1.0000\n",
      "It:270|Loss: 0.4632|accuracy:0.8750\n",
      "It:280|Loss: 0.1239|accuracy:1.0000\n",
      "It:290|Loss: 0.3183|accuracy:0.8750\n",
      "It:300|Loss: 0.1228|accuracy:0.9375\n",
      "It:310|Loss: 0.3798|accuracy:0.8125\n",
      "It:320|Loss: 0.2694|accuracy:0.8125\n",
      "It:330|Loss: 0.0655|accuracy:1.0000\n",
      "It:340|Loss: 0.2888|accuracy:0.8750\n",
      "It:350|Loss: 0.4919|accuracy:0.8750\n",
      "It:360|Loss: 0.2529|accuracy:0.8750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [03:26<00:00, 206.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 1 | val | Loss: 0.2856 | Acc: 0.9050203527815467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ファインチューニング\n",
    "num_epochs = 1\n",
    "my_model_trained = train_model(my_model, dataloader_dict, criterion, optimizer, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b28a2d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 24/24 [00:18<00:00,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "テストデータ737個でのaccuracy: 0.8724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ファインチューニングしたモデルをテストデータで精度評価\n",
    "bt_test = DataLoader(ds_test, batch_size=32)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "my_model_trained.eval()\n",
    "my_model_trained.to(device)\n",
    "\n",
    "epochs_corrects = 0\n",
    "\n",
    "for batch in tqdm(bt_test):\n",
    "    inputs = batch[\"input_ids\"].to(device)\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = my_model_trained(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        epochs_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "epoch_acc = epochs_corrects.double() / (len(bt_test) * bt_test.batch_size)\n",
    "print(f\"テストデータ{len(ds_test)}個でのaccuracy: {epoch_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383d33e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
