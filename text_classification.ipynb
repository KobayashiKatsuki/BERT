{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b8c9029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "BERTを用いたポジネガ分類器つくっちゃうよ\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "from transformers import BertJapaneseTokenizer, BertModel\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, precision_recall_fscore_support\n",
    "\n",
    "from tqdm import tqdm\n",
    "import glob, pickle\n",
    "\n",
    "MODEL_NAME = \"cl-tohoku/bert-base-japanese\"\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e31bfb1",
   "metadata": {},
   "source": [
    "### 事前準備\n",
    "学習データ成形、事前学習クラスのロード、BERTを使ったモデルのクラス作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f94db0f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>当社グループを取り巻く環境は、実質賃金が伸び悩むなか、消費者の皆様の生活防衛意識の高まりや節...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>春から夏にかけましては個人消費の低迷などにより、きのこの価格は厳しい状況で推移いたしました</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>台湾の現地法人「台灣北斗生技股份有限公司」におきましては、ブランドの構築、企画提案などに力を...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>化成品事業におきましては、引き続き厳しい販売環境にありましたが、中核である包装資材部門におき...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>以上の結果、化成品事業の売上高は92億45百万円（同1.7％減）となりました</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2808</th>\n",
       "      <td>当連結会計年度におきましては、連結子会社のデジタル・アドバタイジング・コンソーシアム株式会社...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2809</th>\n",
       "      <td>新規の自動ドアの売上台数は僅かに減少したものの、シートシャッターの大型物件に加え、取替の売上...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2810</th>\n",
       "      <td>加えて、保守契約が堅調に増加し、売上高は6,952百万円（前年同期比1.2％増）となりました</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2811</th>\n",
       "      <td>利益につきましては、取替工事の増加及び保守契約による安定的な利益の確保により、セグメント利益...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2812</th>\n",
       "      <td>その他のセグメントでは駐輪システムが堅調に推移し、売上高は721百万円（前年同期比0.8％増...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2813 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "0     当社グループを取り巻く環境は、実質賃金が伸び悩むなか、消費者の皆様の生活防衛意識の高まりや節...      0\n",
       "1         春から夏にかけましては個人消費の低迷などにより、きのこの価格は厳しい状況で推移いたしました      0\n",
       "2     台湾の現地法人「台灣北斗生技股份有限公司」におきましては、ブランドの構築、企画提案などに力を...      0\n",
       "3     化成品事業におきましては、引き続き厳しい販売環境にありましたが、中核である包装資材部門におき...      0\n",
       "4                以上の結果、化成品事業の売上高は92億45百万円（同1.7％減）となりました      0\n",
       "...                                                 ...    ...\n",
       "2808  当連結会計年度におきましては、連結子会社のデジタル・アドバタイジング・コンソーシアム株式会社...      1\n",
       "2809  新規の自動ドアの売上台数は僅かに減少したものの、シートシャッターの大型物件に加え、取替の売上...      1\n",
       "2810     加えて、保守契約が堅調に増加し、売上高は6,952百万円（前年同期比1.2％増）となりました      1\n",
       "2811  利益につきましては、取替工事の増加及び保守契約による安定的な利益の確保により、セグメント利益...      1\n",
       "2812  その他のセグメントでは駐輪システムが堅調に推移し、売上高は721百万円（前年同期比0.8％増...      1\n",
       "\n",
       "[2813 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ポジネガデータセット\n",
    "df_dataset = pd.read_csv(\n",
    "    'D:/DataSet/chABSA-dataset/chABSA-dataset/dataset.tsv',\n",
    "    sep='\\t', \n",
    "    header=None\n",
    ").rename(columns={0:'text', 1:'label'}).loc[:, ['text', 'label']]\n",
    "\n",
    "# ひとまずこういうFmtのデータに成形するところまでがんばる\n",
    "df_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "545fc1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "https://dreamer-uma.com/pytorch-dataset/\n",
    "\n",
    "対象タスクのデータを扱うDataset\n",
    "データの格納と引き出し　DataLoaderと組み合わせてミニバッチ学習が可能\n",
    "Datasetを自作する場合は必ず以下のメソッドを実装すること\n",
    "__len__(): Datasetのサイズ（データ数）\n",
    "__getitem__(): Datasetの要素にアクセス\n",
    "\n",
    "\"\"\"\n",
    "class PosiNegaDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = { k: torch.tensor(v[idx]) for k, v in self.encodings.items() }\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx]) # item[\"label\"]でなくitem[\"labels\"]が正しい\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "452b523e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "BERTによる分類を行うレイヤのクラス\n",
    "\n",
    "中に事前学習モデルを持ち、\n",
    "input_ids -> model -> output -> classifier\n",
    "という各レイヤのデータフローを流す\n",
    "\"\"\"\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_model):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        \n",
    "        # 事前学習モデル\n",
    "        self.bert = pretrained_model        \n",
    "        # ドロップアウト層（過学習抑止効果）\n",
    "        self.dropout = nn.Dropout(p=.1)\n",
    "        # 線形変換層（全結合層）\n",
    "        # ポジネガ（２カテゴリ）分類なので出力層は2\n",
    "        self.classifier = nn.Linear(in_features=768, out_features=2)\n",
    "        \n",
    "        # 重み初期化\n",
    "        nn.init.normal_(self.classifier.weight, std=0.02)\n",
    "        nn.init.normal_(self.classifier.bias, 0)\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids, \n",
    "        labels, \n",
    "        attention_mask=None, token_type_ids=None, position_ids=None,\n",
    "        head_mask=None, inputs_embeds=None, output_attentions=None,\n",
    "        output_hidden_states=None, return_dict=None\n",
    "    ):\n",
    "        output = self.bert(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids, \n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask, \n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict\n",
    "        )\n",
    "        # トークン毎の特徴量（使わない）\n",
    "        # last_hidden_state = output.last_hidden_state\n",
    "\n",
    "        # 文代表（[CLS]トークン）の特徴量\n",
    "        pooler_output = output.pooler_output\n",
    "        pooler_output = self.dropout(pooler_output)\n",
    "        # 分類タスク\n",
    "        output_classifier = self.classifier(pooler_output)\n",
    "        \n",
    "        # loss計算\n",
    "        loss_func = nn.CrossEntropyLoss()\n",
    "        loss = loss_func(output_classifier.view(-1,2), labels.view(-1))\n",
    "        \n",
    "        # 必ず出力はlossが先\n",
    "        return loss, output_classifier        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9d016d",
   "metadata": {},
   "source": [
    "### 事前準備、以上\n",
    "ここからは上で用意した各種クラスやモデルやデータセットを使ってタスクを解くコーディングをしていく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4255319a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# トークナイザ\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# 事前学習モデル\n",
    "model = BertModel.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6d505ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴量X、ラベルyを取得\n",
    "X, y = df_dataset[\"text\"].values, df_dataset[\"label\"].values\n",
    "\n",
    "# train, val, test分割\n",
    "# random_stateはシャッフルの乱数シード固定、stratifyは正例、負例のラベル数均一にする処理\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.4, random_state=0, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5, random_state=0, stratify=y_val)\n",
    "\n",
    "# トークナイザでモデルへのinputとなるようencodingする\n",
    "max_len = 256 #512\n",
    "\n",
    "enc_train = tokenizer(\n",
    "    X_train.tolist(), \n",
    "    add_special_tokens=True, \n",
    "    max_length=max_len,\n",
    "    padding='max_length',\n",
    "    return_tensors='pt',\n",
    ")\n",
    "# tokenizer.convert_ids_to_tokens(enc_train['attention_masks'].tolist()[0])\n",
    "\n",
    "enc_val = tokenizer(\n",
    "    X_val.tolist(), \n",
    "    add_special_tokens=True, \n",
    "    max_length=max_len,\n",
    "    padding='max_length',\n",
    "    return_tensors='pt',\n",
    ")\n",
    "\n",
    "enc_test = tokenizer(\n",
    "    X_test.tolist(), \n",
    "    add_special_tokens=True, \n",
    "    max_length=max_len,\n",
    "    padding='max_length',\n",
    "    return_tensors='pt',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e26812a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasetを作成\n",
    "ds_train = PosiNegaDataset(enc_train, y_train)\n",
    "ds_val = PosiNegaDataset(enc_val, y_val)\n",
    "ds_test = PosiNegaDataset(enc_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27c867f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\licht\\AppData\\Local\\Temp\\ipykernel_2356\\691864629.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = { k: torch.tensor(v[idx]) for k, v in self.encodings.items() }\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    2,   695,     5,   854,     6,   162,  5329,  6787,  1387,     5,\n",
       "          6624,     7,  5217,  3913,    16,     9,     6,  2735,  1227,  1634,\n",
       "           774,     9,     6,  1216,     5,  5330,    14,  7736,    15,    10,\n",
       "          2442,     5,   881,    28,   130, 17144,    11,  6609,   251,  8025,\n",
       "          3913,    10,    14,     6,  5580,  2040,  1634,   774,     9,     6,\n",
       "          1216,  3279,     9,   837, 28913,     7,  7760,    15,    10,  1036,\n",
       "           780,  3279,     5,  3641,   225,  6857, 28913,     7,  7760,    15,\n",
       "          3913,    10,     3,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'labels': tensor(0)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80fb72f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自作BERTモデル\n",
    "my_model = BertClassifier(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2491de8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 評価関数の設定\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da37bc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainerを作成\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./outputs',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    no_cuda=False,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=50,\n",
    ")\n",
    "\n",
    "if \"trainer\" in locals():\n",
    "    del trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=my_model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=ds_val,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70da72a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\licht\\anaconda3\\envs\\py39\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1687\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 211\n",
      "C:\\Users\\licht\\AppData\\Local\\Temp\\ipykernel_2356\\691864629.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = { k: torch.tensor(v[idx]) for k, v in self.encodings.items() }\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 01:32, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.454600</td>\n",
       "      <td>0.308570</td>\n",
       "      <td>0.886323</td>\n",
       "      <td>0.883410</td>\n",
       "      <td>0.880631</td>\n",
       "      <td>0.887718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.341400</td>\n",
       "      <td>0.248114</td>\n",
       "      <td>0.902309</td>\n",
       "      <td>0.899975</td>\n",
       "      <td>0.896832</td>\n",
       "      <td>0.905309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.224900</td>\n",
       "      <td>0.222127</td>\n",
       "      <td>0.921847</td>\n",
       "      <td>0.918804</td>\n",
       "      <td>0.920053</td>\n",
       "      <td>0.917658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.176700</td>\n",
       "      <td>0.218046</td>\n",
       "      <td>0.927176</td>\n",
       "      <td>0.924803</td>\n",
       "      <td>0.923499</td>\n",
       "      <td>0.926268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 563\n",
      "  Batch size = 32\n",
      "C:\\Users\\licht\\AppData\\Local\\Temp\\ipykernel_2356\\691864629.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = { k: torch.tensor(v[idx]) for k, v in self.encodings.items() }\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 563\n",
      "  Batch size = 32\n",
      "C:\\Users\\licht\\AppData\\Local\\Temp\\ipykernel_2356\\691864629.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = { k: torch.tensor(v[idx]) for k, v in self.encodings.items() }\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 563\n",
      "  Batch size = 32\n",
      "C:\\Users\\licht\\AppData\\Local\\Temp\\ipykernel_2356\\691864629.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = { k: torch.tensor(v[idx]) for k, v in self.encodings.items() }\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 563\n",
      "  Batch size = 32\n",
      "C:\\Users\\licht\\AppData\\Local\\Temp\\ipykernel_2356\\691864629.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = { k: torch.tensor(v[idx]) for k, v in self.encodings.items() }\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=211, training_loss=0.42801548222794916, metrics={'train_runtime': 97.771, 'train_samples_per_second': 17.255, 'train_steps_per_second': 2.158, 'total_flos': 0.0, 'train_loss': 0.42801548222794916, 'epoch': 1.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ファインチューニング\n",
    "%time trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbdd4f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 563\n",
      "  Batch size = 32\n",
      "C:\\Users\\licht\\AppData\\Local\\Temp\\ipykernel_2356\\691864629.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = { k: torch.tensor(v[idx]) for k, v in self.encodings.items() }\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 01:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.2222103774547577,\n",
       " 'eval_accuracy': 0.9271758436944938,\n",
       " 'eval_f1': 0.9247043856930831,\n",
       " 'eval_precision': 0.9238853005521408,\n",
       " 'eval_recall': 0.9255811521062678,\n",
       " 'eval_runtime': 6.3374,\n",
       " 'eval_samples_per_second': 88.837,\n",
       " 'eval_steps_per_second': 2.84,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validationで評価\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a6d3cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 563\n",
      "  Batch size = 32\n",
      "C:\\Users\\licht\\AppData\\Local\\Temp\\ipykernel_2356\\691864629.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = { k: torch.tensor(v[idx]) for k, v in self.encodings.items() }\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.2615799307823181,\n",
       " 'eval_accuracy': 0.9076376554174067,\n",
       " 'eval_f1': 0.9045663172169196,\n",
       " 'eval_precision': 0.9035505267264924,\n",
       " 'eval_recall': 0.9056820856104385,\n",
       " 'eval_runtime': 6.3069,\n",
       " 'eval_samples_per_second': 89.267,\n",
       " 'eval_steps_per_second': 2.854,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testで評価\n",
    "trainer.evaluate(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f358c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル保存\n",
    "torch.save(my_model.state_dict(), \"./outputs/fine-tuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f21ab861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保存済みモデルを読み込む\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "new_model = BertClassifier(model).to(device)\n",
    "new_model.load_state_dict(torch.load(\"./outputs/fine-tuned\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25dd4344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2, 12703,     5,  6376,     9,   121,    19,  1557,  2643,     6,\n",
       "          1452,  9821,    14,  1822,    16,    33,     3,     0],\n",
       "        [    2,   147,  2040,     5,   530,     7,  1320,     6,  9026,     9,\n",
       "          2147,  1337,    11,  2485,    15,  3913,    10,     3],\n",
       "        [    2,  7755,   450,     9, 10821, 17151,    40,  4403, 29065,   212,\n",
       "            16,  1852,     3,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]]), 'labels': tensor([0, 0, 0])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# つかってみる\n",
    "sentences = [\n",
    "    '当社の売り上げは１０年連続減少、経営赤字が続いている',\n",
    "    '新製品の開発に成功、収益は過去最高を達成しました',\n",
    "    'あの映画は本当に面白いからぜひ見てね',    \n",
    "]\n",
    "\n",
    "encodings = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "# ダミーでlabelsを追加\n",
    "# loss計算には使われるが分類には無影響\n",
    "encodings[\"labels\"] = torch.tensor([0 for _ in range(len(sentences))])\n",
    "\n",
    "encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d21f6337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.7724, -1.9646],\n",
       "        [-1.9792,  2.5945],\n",
       "        [-0.1746,  0.8927]], device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = new_model(**encodings)\n",
    "\n",
    "# output は return loss, output_classifier\n",
    "# なので第２項が分類結果\n",
    "output[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e3c67594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9913, 0.0087],\n",
       "        [0.0102, 0.9898],\n",
       "        [0.2559, 0.7441]], device='cuda:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 出力を分類スコアにするためにSoftmaxする\n",
    "softmax_func = nn.Softmax(dim=1)\n",
    "scores = softmax_func(output[1])\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad50fe34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
