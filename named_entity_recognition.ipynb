{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b8c9029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "固有表現抽出\n",
    "\n",
    "\"\"\"\n",
    "import itertools\n",
    "import random\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertJapaneseTokenizer, BertForTokenClassification\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "# 日本語学習済みモデル\n",
    "MODEL_NAME = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f94db0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ＡＢＣ -> ABC\n",
      "ABC -> ABC\n",
      "１２３ -> 123\n",
      "123 -> 123\n",
      "アイウ -> アイウ\n",
      "ｱｲｳ -> アイウ\n"
     ]
    }
   ],
   "source": [
    "normalize = lambda s: unicodedata.normalize(\"NFKC\",s)\n",
    "print(f'ＡＢＣ -> {normalize(\"ＡＢＣ\")}' )  # 全角アルファベット\n",
    "print(f'ABC -> {normalize(\"ABC\")}' )        # 半角アルファベット\n",
    "print(f'１２３ -> {normalize(\"１２３\")}' )  # 全角数字\n",
    "print(f'123 -> {normalize(\"123\")}' )        # 半角数字\n",
    "print(f'アイウ -> {normalize(\"アイウ\")}' )  # 全角カタカナ\n",
    "print(f'ｱｲｳ -> {normalize(\"ｱｲｳ\")}' )        # 半角カタカナ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "545fc1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8-5\n",
    "class NER_tokenizer(BertJapaneseTokenizer):\n",
    "       \n",
    "    def encode_plus_tagged(self, text, entities, max_length):\n",
    "        \"\"\"\n",
    "        [学習時]\n",
    "        学習データ作成用＝文章＋固有表現\n",
    "\n",
    "        文章とそれに含まれる固有表現が与えられた時に、\n",
    "        符号化とラベル列の作成を行う。\n",
    "        \"\"\"\n",
    "        # 固有表現の前後でtextを分割し、それぞれのラベルをつけておく。\n",
    "        # entities は固有表現のリスト\n",
    "        #  name: 固有表現, span: 文章中の位置, type_id: 固有表現タイプのID\n",
    "        entities = sorted(entities, key=lambda x: x['span'][0]) # 固有表現を出現順にソート\n",
    "        splitted = [] # 分割後の文字列を追加していく\n",
    "        position = 0\n",
    "        for entity in entities:\n",
    "            start = entity['span'][0]\n",
    "            end = entity['span'][1]\n",
    "            label = entity['type_id']\n",
    "            # 固有表現ではないものには0のラベルを付与\n",
    "            splitted.append({'text':text[position:start], 'label':0}) \n",
    "            # 固有表現には、固有表現のタイプに対応するIDをラベルとして付与\n",
    "            splitted.append({'text':text[start:end], 'label':label}) \n",
    "            position = end\n",
    "        splitted.append({'text': text[position:], 'label':0})\n",
    "        splitted = [ s for s in splitted if s['text'] ] # 長さ0の文字列は除く\n",
    "        # ex.)\n",
    "        # splitted = [{'text': '昨日の', 'label': 0}, {'text': 'みらい事務所', 'label': 1}, {'text': 'との打ち合わせは順調だった。', 'label': 0}]\n",
    "\n",
    "        # 分割されたそれぞれの文字列をトークン化し、ラベルをつける。\n",
    "        tokens = [] # トークンを追加していく\n",
    "        labels = [] # トークンのラベルを追加していく\n",
    "        for text_splitted in splitted:\n",
    "            text = text_splitted['text']\n",
    "            label = text_splitted['label']\n",
    "            tokens_splitted = self.tokenize(text)\n",
    "            labels_splitted = [label] * len(tokens_splitted)\n",
    "            tokens.extend(tokens_splitted)\n",
    "            labels.extend(labels_splitted)\n",
    "\n",
    "        # 符号化を行いBERTに入力できる形式にする。\n",
    "        input_ids = self.convert_tokens_to_ids(tokens)\n",
    "        encoding = self.prepare_for_model(\n",
    "            input_ids, \n",
    "            max_length=max_length, \n",
    "            padding='max_length', \n",
    "            truncation=True\n",
    "        ) # input_idsをencodingに変換\n",
    "        # 特殊トークン[CLS]、[SEP]のラベルを0にする。\n",
    "        labels = [0] + labels[:max_length-2] + [0] \n",
    "        # 特殊トークン[PAD]のラベルを0にする。\n",
    "        labels = labels + [0]*( max_length - len(labels) ) \n",
    "        encoding['labels'] = labels\n",
    "\n",
    "        return encoding\n",
    "\n",
    "    def encode_plus_untagged(self, text, max_length=None, return_tensors=None):\n",
    "        \"\"\"\n",
    "        [推論時]\n",
    "        推論対象データ作成用＝文章のみ（当然どこが固有表現かは分からない文章）\n",
    "\n",
    "        文章をトークン化し、それぞれのトークンの文章中の位置も特定しておく。\n",
    "        \"\"\"\n",
    "        # 文章のトークン化を行い、\n",
    "        # それぞれのトークンと文章中の文字列を対応づける。\n",
    "        tokens = [] # トークンを追加していく。\n",
    "        tokens_original = [] # トークンに対応する文章中の文字列を追加していく。\n",
    "        words = self.word_tokenizer.tokenize(text) # MeCabで単語に分割\n",
    "        for word in words:\n",
    "            # 単語をサブワードに分割\n",
    "            tokens_word = self.subword_tokenizer.tokenize(word) \n",
    "            tokens.extend(tokens_word)\n",
    "            if tokens_word[0] == '[UNK]': # 未知語への対応\n",
    "                tokens_original.append(word)\n",
    "            else:\n",
    "                tokens_original.extend([\n",
    "                    token.replace('##','') for token in tokens_word\n",
    "                ])\n",
    "\n",
    "        # 各トークンの文章中での位置を調べる。（空白の位置を考慮する）\n",
    "        position = 0\n",
    "        spans = [] # トークンの位置を追加していく。\n",
    "        for token in tokens_original:\n",
    "            l = len(token)\n",
    "            while 1:\n",
    "                if token != text[position:position+l]:\n",
    "                    position += 1\n",
    "                else:\n",
    "                    spans.append([position, position+l])\n",
    "                    position += l\n",
    "                    break\n",
    "\n",
    "        # 符号化を行いBERTに入力できる形式にする。\n",
    "        input_ids = self.convert_tokens_to_ids(tokens) \n",
    "        encoding = self.prepare_for_model(\n",
    "            input_ids, \n",
    "            max_length=max_length, \n",
    "            padding='max_length' if max_length else False, \n",
    "            truncation=True if max_length else False\n",
    "        )\n",
    "        sequence_length = len(encoding['input_ids'])\n",
    "        # 特殊トークン[CLS]に対するダミーのspanを追加。\n",
    "        spans = [[-1, -1]] + spans[:sequence_length-2] \n",
    "        # 特殊トークン[SEP]、[PAD]に対するダミーのspanを追加。\n",
    "        spans = spans + [[-1, -1]] * ( sequence_length - len(spans) ) \n",
    "\n",
    "        # 必要に応じてtorch.Tensorにする。\n",
    "        if return_tensors == 'pt':\n",
    "            encoding = { k: torch.tensor([v]) for k, v in encoding.items() }\n",
    "\n",
    "        return encoding, spans\n",
    "\n",
    "    def convert_bert_output_to_entities(self, text, labels, spans):\n",
    "        \"\"\"\n",
    "        [推論時]\n",
    "        推論結果ラベル列を使って推論対象からの固有表現抽出\n",
    "\n",
    "        文章、ラベル列の予測値、各トークンの位置から固有表現を得る。\n",
    "        \"\"\"\n",
    "        # labels, spansから特殊トークンに対応する部分を取り除く\n",
    "        labels = [label for label, span in zip(labels, spans) if span[0] != -1]\n",
    "        spans = [span for span in spans if span[0] != -1]\n",
    "\n",
    "        # 同じラベルが連続するトークンをまとめて、固有表現を抽出する。\n",
    "        entities = []\n",
    "        for label, group \\\n",
    "            in itertools.groupby(enumerate(labels), key=lambda x: x[1]):\n",
    "            \n",
    "            group = list(group)\n",
    "            start = spans[group[0][0]][0]\n",
    "            end = spans[group[-1][0]][1]\n",
    "\n",
    "            if label != 0: # ラベルが0以外ならば、新たな固有表現として追加。\n",
    "                entity = {\n",
    "                    \"name\": text[start:end],\n",
    "                    \"span\": [start, end],\n",
    "                    \"type_id\": label\n",
    "                }\n",
    "                entities.append(entity)\n",
    "\n",
    "        return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "452b523e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
      "The class this function is called from is 'NER_tokenizer'.\n"
     ]
    }
   ],
   "source": [
    "# 8-6\n",
    "tokenizer = NER_tokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4255319a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      " 'input_ids': [2,\n",
      "               10271,\n",
      "               28486,\n",
      "               5,\n",
      "               546,\n",
      "               10780,\n",
      "               2464,\n",
      "               13,\n",
      "               5,\n",
      "               1878,\n",
      "               2682,\n",
      "               9,\n",
      "               10750,\n",
      "               308,\n",
      "               10,\n",
      "               8,\n",
      "               3,\n",
      "               0,\n",
      "               0,\n",
      "               0],\n",
      " 'labels': [0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "#tokens\n",
      "['[CLS]', '昨', '##日', 'の', 'み', 'らい', '事務所', 'と', 'の', '打ち', '##合わせ', 'は', '順調', 'だっ', 'た', '。', '[SEP]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "# 8-7\n",
    "# 学習データ作成スキームの確認\n",
    "text = '昨日のみらい事務所との打ち合わせは順調だった。'\n",
    "entities = [\n",
    "    {'name': 'みらい事務所', 'span': [3,9], 'type_id': 1}\n",
    "]\n",
    "\n",
    "encoding = tokenizer.encode_plus_tagged(\n",
    "    text, entities, max_length=20\n",
    ")\n",
    "\n",
    "pprint(encoding)\n",
    "\n",
    "print('#tokens')\n",
    "print(tokenizer.convert_ids_to_tokens(encoding['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6d505ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# encoding\n",
      "{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
      " 'input_ids': tensor([[    2,     1, 26280,     5,  1543,   125,     9,  6749, 28550,  2953,\n",
      "         28550, 28566, 21202, 28683, 14050, 12475,    12,    31,     8,     3]]),\n",
      " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "# spans\n",
      "[[-1, -1], [0, 1], [1, 2], [2, 3], [3, 5], [5, 6], [6, 7], [7, 9], [9, 10], [10, 12], [12, 13], [13, 14], [15, 18], [18, 19], [19, 23], [24, 27], [27, 28], [28, 30], [30, 31], [-1, -1]]\n"
     ]
    }
   ],
   "source": [
    "# 8-8\n",
    "# 推論対象データ作成スキームの確認\n",
    "text = '騰訊の英語名はTencent Holdings Ltdである。'\n",
    "encoding, spans = tokenizer.encode_plus_untagged(\n",
    "    text, return_tensors='pt'\n",
    ")\n",
    "print('# encoding')\n",
    "pprint(encoding)\n",
    "print('# spans')\n",
    "print(spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e26812a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': '騰訊', 'span': [0, 2], 'type_id': 1}, {'name': 'Tencent Holdings Ltd', 'span': [7, 27], 'type_id': 1}]\n"
     ]
    }
   ],
   "source": [
    "# 8-9\n",
    "# 推論結果からの固有表現抽出の確認\n",
    "labels_predicted = [0,1,1,0,0,0,0,1,1,1,1,1,1,1,1,1,0,0,0,0]\n",
    "entities = tokenizer.convert_bert_output_to_entities(\n",
    "    text, labels_predicted, spans\n",
    ")\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8ba09c",
   "metadata": {},
   "source": [
    "---\n",
    "### BERTによる固有表現抽出\n",
    "\n",
    "固有表現抽出は、与えられた文章をトークン化し、それぞれのトークンのラベルを予測する分類問題として扱える\\\n",
    "ここではTransformersにおけるトークン単位の分類のためのクラスBertForTokenClassificationを用いてみる\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27c867f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
      "The class this function is called from is 'NER_tokenizer'.\n",
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 8-10\n",
    "tokenizer = NER_tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# num_labelsは、固有表現のtype数+1とする（今回は3タイプなので4）\n",
    "bert_tc = BertForTokenClassification.from_pretrained(\n",
    "    MODEL_NAME, num_labels=4\n",
    ")\n",
    "\n",
    "bert_tc = bert_tc.cuda() #GPUへ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80fb72f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'A', 'span': [0, 1], 'type_id': 1}, {'name': 'さん', 'span': [1, 3], 'type_id': 2}, {'name': 'は', 'span': [3, 4], 'type_id': 1}, {'name': 'B', 'span': [4, 5], 'type_id': 2}, {'name': '大学に入学', 'span': [5, 10], 'type_id': 1}, {'name': 'た。', 'span': [11, 13], 'type_id': 1}]\n"
     ]
    }
   ],
   "source": [
    "# 8-11\n",
    "text = 'AさんはB大学に入学した。'\n",
    "\n",
    "# 符号化を行い、各トークンの文章中での位置も特定しておく。\n",
    "encoding, spans = tokenizer.encode_plus_untagged(\n",
    "    text, return_tensors='pt'\n",
    ") \n",
    "encoding = { k: v.cuda() for k, v in encoding.items() } \n",
    "\n",
    "# BERTでトークン毎の分類スコアを出力し、スコアの最も高いラベルを予測値とする。\n",
    "with torch.no_grad():\n",
    "    output = bert_tc(**encoding)\n",
    "    scores = output.logits\n",
    "    labels_predicted = scores[0].argmax(-1).cpu().numpy().tolist()\n",
    "\n",
    "# ラベル列を固有表現に変換\n",
    "entities = tokenizer.convert_bert_output_to_entities(\n",
    "    text, labels_predicted, spans\n",
    ")\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2491de8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8-12\n",
    "data = [\n",
    "    {\n",
    "        'text': 'AさんはB大学に入学した。',\n",
    "        'entities': [\n",
    "            {'name': 'A', 'span': [0, 1], 'type_id': 2},\n",
    "            {'name': 'B大学', 'span': [4, 7], 'type_id': 1}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'text': 'CDE株式会社は新製品「E」を販売する。',\n",
    "        'entities': [\n",
    "            {'name': 'CDE株式会社', 'span': [0, 7], 'type_id': 1},\n",
    "            {'name': 'E', 'span': [12, 13], 'type_id': 3}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# 各データを符号化し、データローダを作成する。\n",
    "max_length=32\n",
    "dataset_for_loader = []\n",
    "for sample in data:\n",
    "    text = sample['text']\n",
    "    entities = sample['entities']\n",
    "    encoding = tokenizer.encode_plus_tagged(\n",
    "        text, entities, max_length=max_length\n",
    "    )\n",
    "    encoding = { k: torch.tensor(v) for k, v in encoding.items() }\n",
    "    dataset_for_loader.append(encoding)\n",
    "dataloader = DataLoader(dataset_for_loader, batch_size=len(data))\n",
    "\n",
    "# ミニバッチを取り出し損失を得る。\n",
    "for batch in dataloader:\n",
    "    batch = { k: v.cuda() for k, v in batch.items() } # GPU\n",
    "    output = bert_tc(**batch) # BERTへ入力\n",
    "    loss = output.loss # 損失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad50fe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8-14\n",
    "# データのロード\n",
    "dataset = json.load(open('../../DataSet/ner-wikipedia-dataset-main/ner.json','r', encoding='utf-8'))\n",
    "\n",
    "# 固有表現のタイプとIDを対応付る辞書 \n",
    "type_id_dict = {\n",
    "    \"人名\": 1,\n",
    "    \"法人名\": 2,\n",
    "    \"政治的組織名\": 3,\n",
    "    \"その他の組織名\": 4,\n",
    "    \"地名\": 5,\n",
    "    \"施設名\": 6,\n",
    "    \"製品名\": 7,\n",
    "    \"イベント名\": 8\n",
    "}\n",
    "\n",
    "# カテゴリーをラベルに変更、文字列の正規化する。\n",
    "for sample in dataset:\n",
    "    sample['text'] = unicodedata.normalize('NFKC', sample['text'])\n",
    "    for e in sample[\"entities\"]:\n",
    "        e['type_id'] = type_id_dict[e['type']]\n",
    "        del e['type']\n",
    "\n",
    "# データセットの分割\n",
    "random.shuffle(dataset)\n",
    "n = len(dataset)\n",
    "n_train = int(n*0.6)\n",
    "n_val = int(n*0.2)\n",
    "dataset_train = dataset[:n_train]\n",
    "dataset_val = dataset[n_train:n_train+n_val]\n",
    "dataset_test = dataset[n_train+n_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2250c66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
      "The class this function is called from is 'NER_tokenizer'.\n"
     ]
    }
   ],
   "source": [
    "# 8-15\n",
    "def create_dataset(tokenizer, dataset, max_length):\n",
    "    \"\"\"\n",
    "    データセットをデータローダに入力できる形に整形。\n",
    "    \"\"\"\n",
    "    dataset_for_loader = []\n",
    "    for sample in dataset:\n",
    "        text = sample['text']\n",
    "        entities = sample['entities']\n",
    "        encoding = tokenizer.encode_plus_tagged(\n",
    "            text, entities, max_length=max_length\n",
    "        )\n",
    "        encoding = { k: torch.tensor(v) for k, v in encoding.items() }\n",
    "        dataset_for_loader.append(encoding)\n",
    "    return dataset_for_loader\n",
    "\n",
    "# トークナイザのロード\n",
    "tokenizer = NER_tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# データセットの作成\n",
    "max_length = 128\n",
    "dataset_train_for_loader = create_dataset(\n",
    "    tokenizer, dataset_train, max_length\n",
    ")\n",
    "dataset_val_for_loader = create_dataset(\n",
    "    tokenizer, dataset_val, max_length\n",
    ")\n",
    "\n",
    "# データローダの作成\n",
    "dataloader_train = DataLoader(\n",
    "    dataset_train_for_loader, batch_size=32, shuffle=True\n",
    ")\n",
    "dataloader_val = DataLoader(dataset_val_for_loader, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dee6218a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Missing logger folder: D:\\git\\BERT\\lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type                       | Params\n",
      "-------------------------------------------------------\n",
      "0 | bert_tc | BertForTokenClassification | 110 M \n",
      "-------------------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "440.135   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\licht\\anaconda3\\envs\\py39\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 6 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\licht\\anaconda3\\envs\\py39\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 6 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b349c8fd5da945628fca7e01c4b0b05a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 8-16\n",
    "# PyTorch Lightningのモデル\n",
    "class BertForTokenClassification_pl(pl.LightningModule):\n",
    "        \n",
    "    def __init__(self, model_name, num_labels, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.bert_tc = BertForTokenClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels\n",
    "        )\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output = self.bert_tc(**batch)\n",
    "        loss = output.loss\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output = self.bert_tc(**batch)\n",
    "        val_loss = output.loss\n",
    "        self.log('val_loss', val_loss)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_top_k=1,\n",
    "    save_weights_only=True,\n",
    "    dirpath='model/'\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    max_epochs=5,\n",
    "    callbacks=[checkpoint]\n",
    ")\n",
    "\n",
    "# ファインチューニング\n",
    "model = BertForTokenClassification_pl(\n",
    "    MODEL_NAME, num_labels=9, lr=1e-5\n",
    ")\n",
    "trainer.fit(model, dataloader_train, dataloader_val)\n",
    "best_model_path = checkpoint.best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ca769b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
      "The class this function is called from is 'NER_tokenizer'.\n",
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1070/1070 [00:09<00:00, 117.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# 8-17\n",
    "def predict(text, tokenizer, bert_tc):\n",
    "    \"\"\"\n",
    "    BERTで固有表現抽出を行うための関数。\n",
    "    \"\"\"\n",
    "    # 符号化\n",
    "    encoding, spans = tokenizer.encode_plus_untagged(\n",
    "        text, return_tensors='pt'\n",
    "    )\n",
    "    encoding = { k: v.cuda() for k, v in encoding.items() }\n",
    "\n",
    "    # ラベルの予測値の計算\n",
    "    with torch.no_grad():\n",
    "        output = bert_tc(**encoding)\n",
    "        scores = output.logits\n",
    "        labels_predicted = scores[0].argmax(-1).cpu().numpy().tolist() \n",
    "\n",
    "    # ラベル列を固有表現に変換\n",
    "    entities = tokenizer.convert_bert_output_to_entities(\n",
    "        text, labels_predicted, spans\n",
    "    )\n",
    "\n",
    "    return entities\n",
    "\n",
    "# トークナイザのロード\n",
    "tokenizer = NER_tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# ファインチューニングしたモデルをロードし、GPUにのせる。\n",
    "model = BertForTokenClassification_pl.load_from_checkpoint(\n",
    "    best_model_path\n",
    ")\n",
    "bert_tc = model.bert_tc.cuda()\n",
    "\n",
    "# 固有表現抽出\n",
    "# 注：以下ではコードのわかりやすさのために、1データづつ処理しているが、\n",
    "# バッチ化して処理を行った方が処理時間は短い\n",
    "entities_list = [] # 正解の固有表現を追加していく。\n",
    "entities_predicted_list = [] # 抽出された固有表現を追加していく。\n",
    "for sample in tqdm(dataset_test):\n",
    "    text = sample['text']\n",
    "    entities_predicted = predict(text, tokenizer, bert_tc) # BERTで予測\n",
    "    entities_list.append(sample['entities'])\n",
    "    entities_predicted_list.append( entities_predicted )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd6c8843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 正解\n",
      "[{'name': 'ユリ', 'span': [3, 5], 'type_id': 1}, {'name': '衆議院', 'span': [7, 10], 'type_id': 3}, {'name': '米原昶', 'span': [12, 15], 'type_id': 1}]\n",
      "# 抽出\n",
      "[{'name': 'ユリ', 'span': [3, 5], 'type_id': 1}, {'name': '米原昶', 'span': [12, 15], 'type_id': 1}]\n"
     ]
    }
   ],
   "source": [
    "# 8-18\n",
    "print(\"# 正解\")\n",
    "print(entities_list[0])\n",
    "print(\"# 抽出\")\n",
    "print(entities_predicted_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e6681ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8-19\n",
    "def evaluate_model(entities_list, entities_predicted_list, type_id=None):\n",
    "    \"\"\"\n",
    "    正解と予測を比較し、モデルの固有表現抽出の性能を評価する。\n",
    "    type_idがNoneのときは、全ての固有表現のタイプに対して評価する。\n",
    "    type_idが整数を指定すると、その固有表現のタイプのIDに対して評価を行う。\n",
    "    \"\"\"\n",
    "    num_entities = 0 # 固有表現(正解)の個数\n",
    "    num_predictions = 0 # BERTにより予測された固有表現の個数\n",
    "    num_correct = 0 # BERTにより予測のうち正解であった固有表現の数\n",
    "\n",
    "    # それぞれの文章で予測と正解を比較。\n",
    "    # 予測は文章中の位置とタイプIDが一致すれば正解とみなす。\n",
    "    for entities, entities_predicted \\\n",
    "        in zip(entities_list, entities_predicted_list):\n",
    "\n",
    "        if type_id:\n",
    "            entities = [ e for e in entities if e['type_id'] == type_id ]\n",
    "            entities_predicted = [ \n",
    "                e for e in entities_predicted if e['type_id'] == type_id\n",
    "            ]\n",
    "            \n",
    "        get_span_type = lambda e: (e['span'][0], e['span'][1], e['type_id'])\n",
    "        set_entities = set( get_span_type(e) for e in entities )\n",
    "        set_entities_predicted = \\\n",
    "            set( get_span_type(e) for e in entities_predicted )\n",
    "\n",
    "        num_entities += len(entities)\n",
    "        num_predictions += len(entities_predicted)\n",
    "        num_correct += len( set_entities & set_entities_predicted )\n",
    "\n",
    "    # 指標を計算\n",
    "    precision = num_correct/num_predictions # 適合率\n",
    "    recall = num_correct/num_entities # 再現率\n",
    "    f_value = 2*precision*recall/(precision+recall) # F値\n",
    "\n",
    "    result = {\n",
    "        'num_entities': num_entities,\n",
    "        'num_predictions': num_predictions,\n",
    "        'num_correct': num_correct,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f_value': f_value\n",
    "    }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53f91a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_entities': 2664, 'num_predictions': 2738, 'num_correct': 2292, 'precision': 0.8371073776479182, 'recall': 0.8603603603603603, 'f_value': 0.8485746019992596}\n"
     ]
    }
   ],
   "source": [
    "# 8-20\n",
    "print( evaluate_model(entities_list, entities_predicted_list) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978460c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
